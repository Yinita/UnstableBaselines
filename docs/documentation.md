# Unstable Baselines Documentation

> **Version:** 0.1 ¬∑ **Last Updated:** 2025-06-21

---

## Table of Contents

1. [Introduction](#introduction)

2. [Getting Started](#getting-started)
   * Installation
   * Quick Start

3. [Architecture Overview](#architecture-overview)

4. [Core Modules](#core-modules)
   * [Actor (`actor.py`)](#actor)
   * [Collector (`collector.py`)](#collector)
   * [Model Pool (`model_pool.py`)](#model-pool)
   * [Step Buffer (`buffer.py`)](#step-buffer)
   * [Learner (`learners/standard_learner.py`)](#learner)
   * [Tracker (`trackers.py`)](#tracker)
   * [Terminal Interface (`terminal_interface.py`)](#terminal-interface)
   * [Core Data Structures (`core.py`)](#core-data-structures)

5. [Reward Transformations](#reward-transformations)
   * Final Reward
   * Step Reward
   * Sampling Reward

6. [Algorithms](#algorithms)
   * Reinforce (`algorithms/reinforce.py`)
   * Extending with Custom Algorithms

7. [Utilities and Helpers](#utilities-and-helpers)
   * Templates (`utils/templates.py`)
   * Logging (`utils/logging.py`)

8. [Configuration Reference](#configuration-reference)

9. [Extending the Framework](#extending-the-framework)

10. [Troubleshooting & FAQ](#troubleshooting-and-faq)

11. [Contributing](#contributing)

12. [Contact & Support](#contact-and-support)

---

## Introduction

Brief overview of Unstable Baselines, goals, and main features.

---

## Getting Started

### Installation

Instructions for setting up the environment and dependencies.

### Quick Start

A simple example to immediately run and validate the setup.

---

## Architecture Overview

Visual and textual descriptions of system architecture and workflow.

---

# Core Modules

Below is a high‚Äëlevel index of every core component in **Unstable‚ÄØBaselines**. Click any row (or the ‚ñ∏ icon) to expand its full reference.

| Module                   | Source File                    | One‚Äëline Purpose                                   |
| ------------------------ | ------------------------------ | -------------------------------------------------- |
| **VLLMActor**            | `actor.py`                     | GPU‚Äëbound async text generation + LoRA hot‚Äëswap    |
| **Collector**            | `collector.py`                 | Orchestrates episode rollout & trajectory capture  |
| **ModelPool**            | `model_pool.py`                | Checkpoint registry, ELO scores, opponent sampling |
| **StepBuffer**           | `buffer.py`                    | Replay buffer & prioritised sampling               |
| **Learner**              | `learners/standard_learner.py` | PPO / REINFORCE optimiser & weight sync            |
| **Tracker**              | `trackers.py`                  | Centralised metrics & experiment logging           |
| **TerminalInterface**    | `terminal_interface.py`        | Lightweight CLI dashboard                          |
| **Core¬†Data¬†Structures** | `core.py`                      | `Trajectory`, `EpisodeResult`, etc. schema         |

---

<details>
<summary><strong>VLLMActor¬†(`actor.py`)</strong></summary>

## `VLLMActor` ‚Äî *actor.py*

Asynchronous, Ray‚Äëbased wrapper around a single **vLLM** engine instance.
Receives text‚Äëgeneration requests, batches them on a GPU, supports **LoRA** hot‚Äëswapping, and reports rich throughput metrics.

### Parameters

| Name      | Type                    | Meaning                                              |
| --------- | ----------------------- | ---------------------------------------------------- |
| `cfg`     | `Dict[str, Any]`        | Parsed YAML/CLI configuration (selected keys below). |
| `tracker` | `ray.actor.ActorHandle` | Central metrics sink.                                |
| `name`    | `str`                   | Human‚Äëreadable tag used in logs & dashboards.        |

| **`cfg` keys consumed here**       | Purpose                                           |
| ---------------------------------- | ------------------------------------------------- |
| `model_name`                       | Base model (HF id or local path).                 |
| `max_loras`                        | Maximum resident LoRA adapters (GPU + CPU).       |
| `lora_config.lora_rank`            | Rank for each adapter.                            |
| `max_parallel_seq`                 | Upper bound on concurrent sequences per `step()`. |
| `max_model_len`                    | Context length.                                   |
| `temperature / top_p / max_tokens` | Sampling hyper‚Äëparameters.                        |

### Attributes

* **`engine`**¬†`vllm.LLMEngine`¬†‚Äì underlying generator initialised from **EngineArgs**.
* **`sampling_params`**¬†`vllm.SamplingParams`¬†‚Äì immutable settings shared by every request.
* **`submit_prompt()`** ‚Äì awaitable API entry‚Äëpoint.
* **`_batch_loop()`** ‚Äì background task that drains the queue and calls `engine.step()`.
* **`_report_loop()`** ‚Äì background task that sends queue / TPS metrics to *Tracker* every¬†5‚ÄØs.
* **`_tok_rate()`** ‚Äì helper for rolling tokens‚Äëper‚Äësecond.

### Runtime Lifecycle

1. **`submit_prompt`** ‚Äì queues *(prompt, lora)* pair; returns an `asyncio.Future`.
2. **`_batch_loop`** ‚Äì every¬†20‚ÄØms drains the queue, adds requests to vLLM, calls `engine.step()`, timestamps new tokens for TPS, fulfils finished futures.
3. **`_report_loop`** ‚Äì every¬†5‚ÄØs logs & forwards `{queued,running,tok_s}` to *Tracker*.
4. **Shutdown** ‚Äì cancelling the Ray actor stops both background tasks gracefully.

### Public API Summary

| Method          | Signature                                                     | Purpose                                                                |
| --------------- | ------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `submit_prompt` | `async (prompt: str, lora_path: Optional[str] = None) -> str` | Enqueue a generation job and await the resulting text.                 |
| `_tok_rate`     | `(window: float = 2.0) -> float`                              | Rolling tokens‚Äëper‚Äësecond over *window*¬†s (internal, handy for tests). |

</details>

<details>
<summary><strong>Collector¬†(`collector.py`)</strong></summary>

## `Collector` ‚Äî *collector.py*

Ray actor responsible for orchestrating self‚Äëplay **training** episodes and fixed‚Äëopponent **evaluation** episodes. It routes finished trajectories to the learner‚Äôs **StepBuffer**, maintains ELO scores via **ModelPool**, and logs everything through **Tracker**.

### Responsibilities

* Spawns `num_actors` GPU workers (`VLLMActor`) and assigns episodes round‚Äërobin.
* Samples training & evaluation environments/opponents.
* Submits remote `play_episode` tasks, tracks them in `flight`, and handles results.
* Streams trajectories to **StepBuffer**, pushes game outcomes to **ModelPool**, and records metrics via **Tracker**.

### Constructor Arguments

| Name                      | Type                                | Purpose                                        |
| ------------------------- | ----------------------------------- | ---------------------------------------------- |
| `num_actors`              | `int`                               | How many `VLLMActor` GPUs to spawn.            |
| `step_buffer`             | `ray.actor.ActorHandle`             | Remote buffer storing raw steps.               |
| `model_pool`              | `ray.actor.ActorHandle`             | Checkpoint registry & ELO logic.               |
| `tracker`                 | `BaseTracker`                       | Central experiment logger.                     |
| `vllm_config`             | `dict`                              | Config forwarded to each `VLLMActor`.          |
| `training_envs`           | `list[(env_id, players, template)]` | Candidate envs for self‚Äëplay.                  |
| `evaluation_envs`         | `list[(env_id, players, template)]` | Candidate envs for offline eval.               |
| `evaluation_opponent`     | `str`                               | Fixed opponent HF / OpenRouter model.          |
| `max_eval_games_per_ckpt` | `int`                               | Cap evaluation episodes per checkpoint √ó env.  |
| `filter_opponent_invalid` | `bool`                              | Drop games ended by opponent invalid.          |
| `action_extraction`       | `str`                               | Key selecting extraction/formatting functions. |

### Key Methods

| Method                                   | Purpose                                                                            |
| ---------------------------------------- | ---------------------------------------------------------------------------------- |
| `collect(num_workers, num_eval_workers)` | Main loop: keeps *num\_workers* train & *num\_eval\_workers* eval tasks in flight. |
| `_submit_train()`                        | Launches a training episode with a sampled opponent.                               |
| `_submit_eval(ckpt_uid)`                 | Launches an evaluation episode against the fixed opponent.                         |
| `_handle_finished(ref)`                  | Processes a completed `play_episode`; delegates to `_post_train/_post_eval`.       |
| `_post_train` / `_post_eval`             | Push trajectory / eval reward to downstream subsystems.                            |

### Episode Flow

1. **Spec creation** ‚Äì build `PlaySpec` describing env, players, checkpoint paths & seeds.
2. **Remote rollout** ‚Äì `play_episode.remote(spec, actor)` executes the full loop off‚Äëprocess.
3. **Result handling** ‚Äì finished futures are popped from `flight`; data streamed to buffers & loggers.
4. **Back‚Äëpressure** ‚Äì honours `StepBuffer.continue_collection()` to pause when buffer is near capacity.

### Practical Tips

* Increase `num_eval_workers` if evaluation becomes a bottleneck.
* Enable `filter_opponent_invalid` in competitive settings to ignore wins by opponent invalid move.
* Separate `training_envs` & `evaluation_envs` to avoid evaluator leakage.

</details>

<details>
<summary><strong>ModelPool (`model_pool.py`)</strong></summary>

## `ModelPool` ‚Äî *model\_pool.py*

Central registry and rating system for **all opponents**: learner checkpoints and fixed baseline models.
Maintains **TrueSkill** ratings, exploration statistics, opponent sampling logic, and enforces a VRAM‚Äëfriendly cap on active LoRA adapters.

### Constructor Arguments

| Name              | Type                            | Purpose                                               |
| ----------------- | ------------------------------- | ----------------------------------------------------- |
| `sample_mode`     | `str`                           | Opponent selection strategy (see *Sampling Modes*).   |
| `max_active_lora` | `int`                           | Max number of checkpoint LoRAs flagged `active=True`. |
| `tracker`         | `ray.actor.ActorHandle \| None` | Optional tracker for dashboard snapshots.             |
| `lag_range`       | `(int,int)`                     | Low/high indices used by the *lagged* strategy.       |

### Responsibilities

* **Checkpoint registry** ‚Äì `add_checkpoint()` logs a new UID, carries forward Œº/œÉ.
* **Fixed opponents** ‚Äì `add_fixed()` registers static baselines (no checkpoints).
* **Opponent sampling** ‚Äì `sample(uid_me)` implements 6+ heuristics.
* **Rating updates** ‚Äì `push_game_outcome()` calls `_update_ratings()` and `_register_game()`.
* **Exploration metrics** ‚Äì Tracks state‚Äëspace coverage via `ExplorationTracker`.
* **LoRA pool maintenance** ‚Äì `_maintain_active_pool()` flips `Opponent.active` flags to honor `max_active_lora`.
* **Snapshotting** ‚Äì `snapshot()` pushes a JSON‚Äëserialisable view to *Tracker* for later analysis.

### Key Methods

| Method                                                                 | Returns                | Summary                                            |
| ---------------------------------------------------------------------- | ---------------------- | -------------------------------------------------- |
| `current_uid()`                                                        | `str \| None`          | UID of the latest learner checkpoint.              |
| `latest_ckpt()`                                                        | `str \| None`          | Alias for `current_uid()`.                         |
| `ckpt_path(uid)`                                                       | `(path, kind) \| None` | Resolve a UID to (filesystem path, kind).          |
| `sample(uid_me)`                                                       | `str`                  | Choose an opponent UID according to `sample_mode`. |
| `push_game_outcome(uid_me, uid_opp, final_reward, action_seq, env_id)` | ‚Äî                      | Update ratings & exploration, then snapshot state. |

### Sampling Modes

| Mode            | Logic                                                       |         |                                     |
| --------------- | ----------------------------------------------------------- | ------- | ----------------------------------- |
| `fixed`         | Uniform random among fixed baselines only.                  |         |                                     |
| `mirror`        | Returns the current learner checkpoint (self‚Äëplay).         |         |                                     |
| `lagged`        | Uniform among *active* past checkpoints inside `lag_range`. |         |                                     |
| `random`        | Uniform over fixed + active checkpoints.                    |         |                                     |
| `match-quality` | Softmax based on `TrueSkill.quality()` vs. `uid_me`.        |         |                                     |
| `ts-dist`       | Softmax over                                                | Œº‚òÖ‚ÄìŒºopp | (smaller distance ‚áí higher weight). |
| `exploration`   | Placeholder: rank opponents by expected state diversity.    |         |                                     |

### Rating Update Formula

For a finished game with reward *r¬†‚àà¬†{‚Äë1,¬†0,¬†1}* (win/draw/loss for *learner*):

```python
if r == 1:
    new_a, new_b = TS.rate_1vs1(a, b)      # learner wins
elif r == -1:
    new_b, new_a = TS.rate_1vs1(b, a)      # learner loses
else:
    new_a, new_b = TS.rate_1vs1(a, b, drawn=True)
```

Œº/œÉ are then written back into `self._models`.

### Practical Tips

* **Keep `max_active_lora` small** (‚â§4) when GPUs are scarce; inactive checkpoints can still be sampled as *fixed* opponents via OpenRouter.
* Switch to **`match-quality`** after a few hundred games to keep training pairs evenly matched.
* Call **`add_fixed()`** early so baseline ratings converge before checkpoints appear.
* The **`exploration`** mode is experimental‚ÄîPRs are welcome!

</details>

<details>
<summary><strong>StepBuffer (`buffer.py`)</strong></summary>

## `StepBuffer` ‚Äî *buffer.py*

High‚Äëthroughput **step‚Äëlevel** replay buffer that lives on a Ray actor.
Stores `Step` objects emitted from complete game trajectories, applies configurable reward transformations, downsamples when full, and serves randomised **training batches** to the learner.

### Constructor Arguments

| Name                             | Type                                      | Purpose                                                |
| -------------------------------- | ----------------------------------------- | ------------------------------------------------------ |
| `max_buffer_size`                | `int`                                     | Hard cap on number of `Step` objects kept in memory.   |
| `tracker`                        | `BaseTracker`                             | Logger for buffer metrics & CSV dumps.                 |
| `final_reward_transformation`    | `ComposeFinalRewardTransforms \| None`    | Optional pipeline applied to end‚Äëof‚Äëgame rewards.      |
| `step_reward_transformation`     | `ComposeStepRewardTransforms \| None`     | Optional function applied at each step (shaping).      |
| `sampling_reward_transformation` | `ComposeSamplingRewardTransforms \| None` | Optional post‚Äëprocessing applied *only when sampling*. |
| `buffer_strategy`                | `str`                                     | Currently only `"random"` (uniform reservoir).         |

### Responsibilities

* **Trajectory ingestion** ‚Äì `add_trajectory()` unrolls a `Trajectory` into individual `Step`s and stores them.
* **Reward shaping** ‚Äì applies the supplied transformation pipelines at *final* and *step* granularity.
* **Capacity management** ‚Äì once `len(steps) > max_buffer_size`, uniformly removes excess samples.
* **Batch provisioning** ‚Äì `get_batch(batch_size)` uniform random‚Äësamples *without replacement*, applies optional `sampling_reward_transformation`, and returns the list.
* **Book‚Äëkeeping** ‚Äì CSV dumps of each batch and buffer‚Äësize logging for easy debugging.

### Key Methods

| Method                                          | Returns      | Summary                                                       |
| ----------------------------------------------- | ------------ | ------------------------------------------------------------- |
| `add_trajectory(trajectory, player_id, env_id)` | ‚Äî            | Flattens a finished trajectory into `Step`s and appends them. |
| `get_batch(batch_size)`                         | `List[Step]` | Pop *batch\_size* random steps; writes a CSV snapshot.        |
| `clear()`                                       | ‚Äî            | Purge all stored steps.                                       |
| `stop()`                                        | ‚Äî            | Set `collect=False` so Collector pauses ingestion.            |
| `size()`                                        | `int`        | Current number of stored steps.                               |
| `continue_collection()`                         | `bool`       | Helper polled by Collector for back‚Äëpressure.                 |

### Reward Transformation Hooks

* **Final reward** ‚Äì `ComposeFinalRewardTransforms` maps the *vector* of per‚Äëplayer rewards to a new vector (e.g., win ‚Üí +1 / loss ‚Üí ‚Äì1).
* **Step reward** ‚Äì called for each step *i* with `(trajectory, step_index=i, base_reward)`; enables shaped rewards like dense progress signals.
* **Sampling reward** ‚Äì run on the *batch* right before returning; useful for on‚Äëpolicy advantages or normalisation.

### Capacity Workflow

```text
add_trajectory()
  ‚îú‚îÄ‚îÄ append new steps
  ‚îî‚îÄ‚îÄ if len(steps) > max_buffer_size:
        random.sample(excess) ‚Üí steps.remove()
```

This simple uniform reservoir keeps memory bounded while preserving sample diversity.

### Practical Tips

* **Disk snapshots** ‚Äì batches are written to `<train_dir>/train_data_step_<N>.csv`; disable by monkey‚Äëpatching `write_training_data_to_file`.
* **Prioritised replay** ‚Äì implement a new `buffer_strategy` (e.g., PER) and replace the random down‚Äësampling / sampling logic.
* When training becomes I/O‚Äëbound, consider moving CSV writes onto a background thread or disabling them in production.

</details>

<details>
<summary><strong>Learner (`learners/standard_learner.py`)</strong></summary>

## `StandardLearner` ‚Äî *learners/standard\_learner.py*

Main **parameter‚Äëupdating** component. Consumes `Step` batches from **StepBuffer**, computes policy‚Äëgradient losses via a pluggable `BaseAlgo` (e.g., PPO, REINFORCE) and writes **LoRA checkpoints** every *N* steps. Also registers each new checkpoint with **ModelPool** so it can be sampled as an opponent.

### Constructor Arguments

| Name                       | Type          | Purpose                                              |
| -------------------------- | ------------- | ---------------------------------------------------- |
| `model_name`               | `str`         | HF id or local path of the *base* model.             |
| `step_buffer`              | `StepBuffer`  | Source of training data batches.                     |
| `model_pool`               | `ModelPool`   | Destination for newly‚Äëminted checkpoints.            |
| `algorithm`                | `BaseAlgo`    | Policy‚Äëgradient implementation (PPO, etc.).          |
| `batch_size`               | `int`         | Number of `Step`s per learner update.                |
| `mini_batch_size`          | `int`         | Sub‚Äëdivision for gradient accumulation.              |
| `max_generation_len`       | `int`         | Truncation length during rollouts.                   |
| `learning_rate`            | `float`       | AdamW learning rate (LoRA params only).              |
| `grad_clip`                | `float`       | Global **L2‚Äënorm** gradient clip.                    |
| `batch_delay_buffer`       | `float`       | Multiplier controlling back‚Äëpressure on buffer.      |
| `lora_cfg`                 | `dict`        | LoRA rank, Œ±, dropout, etc.                          |
| `initial_lora_path`        | `str \| None` | Warm‚Äëstart from a prior adapter.                     |
| `num_learners`             | `int`         | How many concurrent learners share the buffer.       |
| `ckpt_root`                | `str`         | Directory for saving checkpoints.                    |
| `save_every`               | `int`         | Save+register every *N* learner steps.               |
| `activation_checkpointing` | `bool`        | Enable full activation CKPT to save VRAM.            |
| `gradient_checkpointing`   | `bool`        | Enable HF gradient CKPT.                             |
| `use_trainer_cache`        | `bool`        | Keep model KV cache during fwd pass (speed vs. RAM). |
| `max_train_len`            | `int \| None` | Hard limit on token count seen by loss fn.           |

### Training Loop (`train(iterations)`) ‚Äî High‚Äëlevel Steps

1. **Wait for data** ‚Äì block until `StepBuffer.size() ‚â• batch_size √ó batch_delay_buffer`.
2. **Fetch batch** ‚Äì `get_batch(batch_size)` returns uniform random `Step`s.
3. **Gradient accumulation** ‚Äì split into `mini_batch_size` chunks; call `algorithm.update()` under `torch.autocast(bfloat16)`.
4. **Clip & step** ‚Äì global L2 clipping then `optimizer.step()`.
5. **Logging** ‚Äì aggregate metrics, grad norm, LR; push to **Tracker**.
6. **Checkpoint** ‚Äì every *save\_every* steps, write LoRA adapter to disk and `ModelPool.add_checkpoint()`.
7. **Stop‚Äëcondition** ‚Äì once `self._step == iterations`, stop buffer collection.

### Attributes Exposed to Other Actors

| Attribute       | Type                               | Description                                        |
| --------------- | ---------------------------------- | -------------------------------------------------- |
| `device`        | `torch.device`                     | CUDA / CPU device resolved from Ray GPU placement. |
| `model`         | `transformers.PreTrainedModel`     | PEFT‚Äëwrapped policy network.                       |
| `tokenizer`     | `transformers.PreTrainedTokenizer` | Matching tokenizer for `model`.                    |
| `_step`         | `int`                              | Learner update counter.                            |
| `_samples_seen` | `int`                              | Cumulative number of `Step`s consumed.             |

### PEFT & Memory Optimisations

* **LoRA‚Äëonly training** keeps GPU memory low; base weights are frozen by default.
* `enable_full_activation_ckpt()` wraps each module in `torch.utils.checkpoint` ‚Äî expect \~20‚Äë30‚ÄØ% slower fwd pass but ‚â§50‚ÄØ% VRAM.
* Set `torch.set_default_dtype(torch.bfloat16)` and `torch.set_float32_matmul_precision('high')` for Ampere+ GPUs.

### Practical Tips

* **Throughput** ‚Äì choose `batch_delay_buffer ‚âà 1.5‚Äì2.0`; higher values reduce idle GPU time.
* **Stability** ‚Äì if loss spikes, reduce `learning_rate` or increase `grad_clip`.
* **Checkpoint hygiene** ‚Äì old adapters can be pruned offline; `ModelPool` only keeps `max_active_lora` in VRAM.
* **Multiple learners** ‚Äì set `num_learners > 1` only when you shard the buffer; otherwise they‚Äôll compete for samples.

</details>

<details>
<summary><strong>Tracker (`trackers.py`)</strong></summary>

## `Tracker` ‚Äî *trackers.py*

Central **metrics bus** for the entire pipeline. Runs as a lightweight Ray
actor, buffers scalar logs in‚Äëmemory, aggregates them into means, and
periodically pushes the result to **Weights¬†&¬†Biases** (optional) and to
the interactive terminal UI.

### Constructor Arguments

| Name            | Type          | Purpose                                                                                               |
| --------------- | ------------- | ----------------------------------------------------------------------------------------------------- |
| `run_name`      | `str`         | Display name for the current experiment.                                                              |
| `wandb_project` | `str \| None` | If supplied, `wandb.init(project=‚Ä¶, name=run_name)` is called and every flush uploads a metrics dict. |

### Internal State

| Attribute          | Purpose                                                      |
| ------------------ | ------------------------------------------------------------ |
| `FLUSH_EVERY`      | Seconds between *automatic* flushes (default¬†64¬†s).          |
| `_m`               | `defaultdict(str‚Üídeque)` raw per‚Äëkey history (‚â§512 entries). |
| `_buffer`          | Current *aggregated* snapshot that will be flushed.          |
| `_n`               | Per‚Äëprefix counters (e.g., number of trajectories logged).   |
| `_interface_stats` | Cached dict used by the **TerminalInterface**.               |
| `use_wandb`        | Bool gate so the actor works offline too.                    |

### Responsibilities

* **Aggregation** ‚Äì store every scalar via `_put(k,v)`; compute means with `_agg(prefix)`.
* **Time‚Äëbased flushing** ‚Äì `_flush_if_due()` fires when `time.monotonic()¬†‚Äì¬†_last_flush >= FLUSH_EVERY`.
* **Metric namespaces** ‚Äì prefixes encode data sources:

  * `collection‚Äë<env_id>/‚Ä¶` ‚Äì training trajectories.
  * `evaluation‚Äë<env_id>/‚Ä¶` ‚Äì offline evaluation.
  * `inference/<actor>/‚Ä¶` ‚Äì GPU token/sec + queue stats.
  * `learner/‚Ä¶` ‚Äì loss, grad norm, samples seen.
* **Model‚Äëpool introspection** ‚Äì `log_model_pool()` dumps TrueSkill, exploration % and match counts into the dashboard.
* **Terminal feed** ‚Äì `get_interface_info()` returns a compact dict used by the curses‚Äëstyle UI.

### Key Public Methods

| Method                                               | Summary                                                          |
| ---------------------------------------------------- | ---------------------------------------------------------------- |
| `add_trajectory(traj, player_id, env_id)`            | Logs reward, win‚Äërate, formatting success, game length, etc.     |
| `add_eval_episode(rewards, player_id, env_id)`       | Logs evaluation reward & outcome.                                |
| `log_inference(actor, gpu_ids, stats)`               | Ingests throughput stats from every `VLLMActor`.                 |
| `log_learner(info)`                                  | Single‚Äëcall log for each learner step (losses, LR, grad norm).   |
| `log_model_pool(match_counts, ts_dict, exploration)` | Records pool‚Äëlevel data (TrueSkill Œº/œÉ, unique n‚Äëgram coverage). |
| `get_interface_info()`                               | Returns dict consumed by **TerminalInterface**.                  |

### Flush Cycle

```text
‚îå every scalar arrives via any log_* method ‚îê
‚îÇ  _put(key, value)                        ‚îÇ
‚îî‚îÄ‚îÄ‚ñ∫ _buffer.update(_agg(prefix))          ‚îÇ
            ‚îÇ                              ‚îÇ
            ‚îî‚îÄ‚îÄ‚ñ∫ _flush_if_due() ‚îÄ‚îÄ‚ñ∫ wandb.log(_buffer) every 64¬†s
```

### Practical Tips

* **Offline mode** ‚Äì omit `wandb_project` to disable WANDB completely; metrics remain query‚Äëable via `TerminalInterface`.
* **Custom scalars** ‚Äì any key that starts with an existing prefix will
  be averaged automatically; no schema changes required.
* **Adjust cadence** ‚Äì set `Tracker.FLUSH_EVERY = 30` before launching if
  you prefer faster WANDB updates.
* **Derived metrics** ‚Äì compute heavy stats offline; push them via
  `log_model_pool()` rather than inside the tight game loop.

</details>

<details>
<summary><strong>TerminalInterface¬†(`terminal_interface.py`)</strong></summary>

*Documentation forthcoming‚Ä¶*

</details>

<details>
<summary><strong>Core¬†Data¬†Structures¬†(`core.py`)</strong></summary>

## Key Dataclasses

| Name                       | Fields                                                                                    | Purpose                                                          |
| -------------------------- | ----------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
| **`Trajectory`**           | `pid, obs, actions, extracted_actions, infos, final_rewards, num_turns, format_feedbacks` | Full record of a *single* game episode from one agent‚Äôs POV.     |
| **`Step`**                 | `pid, obs, act, reward, env_id, step_info`                                                | Flattened, per‚Äëturn training sample passed to **Learner**.       |
| **`Opponent`**             | `uid, kind, path_or_name, rating, active`                                                 | Metadata + TrueSkill rating for every opponent in **ModelPool**. |
| **`EpisodeResult`**        | `traj, end_by_opponent_invalid, action_seq, final_rewards`                                | Light‚Äëweight wrapper returned by `play_episode()`.               |
| **`PlaySpec`** *(frozen)*  | `env_id, num_players, player_id, agent_specs, seed`                                       | Declarative description used to spawn a rollout.                 |
| **`AgentSpec`** *(frozen)* | `kind, model, prompt_template, action_extraction_fn`                                      | Specifies how each player should act inside `play_episode`.      |
| **`TaskMeta`**             | `type, env_id, player_id, seed, ckpt_uid, opponent_uid`                                   | Book‚Äëkeeping blob attached to every in‚Äëflight rollout.           |

### Utility Classes

* **`BaseAlgo`** ‚Äì abstract interface for policy‚Äëgradient algorithms (`initialize`, `prepare_batch`, `update`).
* **`BaseTracker`** ‚Äì filesystem helper that exposes output directories (train / eval / checkpoints / logs).
* **`ExplorationTracker`** ‚Äì rolling window *n‚Äëgram* coverage metric used by **ModelPool** to encourage diverse opponents.

### Example ‚Äî Building a Custom Dataclass

Need a new structure (e.g., to log curiosity bonuses)? Simply import `dataclass` and extend:

```python
from dataclasses import dataclass

@dataclass
class CuriosityStep:
    pid: int
    obs: str
    act: str
    reward: float
    curiosity: float  # üëà your extra field
```

`Learner.prepare_batch()` can then branch on `isinstance(step, CuriosityStep)`.

</details>

---

# Reward¬†Transformations

Below utilities live under `unstable/reward_transformations/`. They let you
reshape sparse win‚Äëloss rewards into *denser* learning signals or correct
for known biases (e.g., first‚Äëplayer advantage).

<details>
<summary><strong>Final‚ÄëReward¬†Transforms¬†(`transformation_final.py`)</strong></summary>

### API

* Every transform inherits from **`FinalRewardTransform`** and implements
  `__call__(x: Dict[int, float], env_id: str|None) -> Dict[int, float]`.
* A stack is built via **`ComposeFinalRewardTransforms([...])`**; transforms
  are applied *sequentially*.

### Built‚Äëin Transforms

| Class                         | Effect                                             |
| ----------------------------- | -------------------------------------------------- |
| `WinDrawLossFormatter`        | Maps raw score *s* ‚Üí `{‚Äë1,0,1}` win/draw/loss.     |
| `RoleAdvantageFormatter`      | Subtracts an EMA of each role‚Äôs historical reward. |
| `RoleAdvantageByEnvFormatter` | Same, but tracked per‚Äëenvironment ID.              |

### Custom Transform Example

```python
class ScaleRewardTransform(FinalRewardTransform):
    """Multiply every reward by *alpha*."""
    def __init__(self, alpha: float = 0.1):
        self.alpha = alpha
    def __call__(self, x, env_id=None):
        return {pid: r * self.alpha for pid, r in x.items()}

# Register it:
transforms = ComposeFinalRewardTransforms([
    WinDrawLossFormatter(),
    ScaleRewardTransform(alpha=0.2),
])
```

This scales the usual `{‚Äë1,0,1}` output down to `{‚Äë0.2,0,0.2}`.

</details>

<details>
<summary><strong>Step‚ÄëReward¬†Transforms¬†(`transformation_step.py`)</strong></summary>

### API

* Implement **`StepRewardTransform`** with `__call__(trajectory, step_index, base_reward) -> float`.
* Chain them with **`ComposeStepRewardTransforms([...])`**; each transform receives the output of the previous one.

### Built‚Äëin Transforms

| Class                   | Effect                                                                           |
| ----------------------- | -------------------------------------------------------------------------------- |
| `RewardForFormat`       | Adds `reward` if the agent‚Äôs answer is well‚Äëformatted; otherwise adds `penalty`. |
| `PenaltyForInvalidMove` | Adds `penalty` when the agent commits an invalid move; otherwise adds `reward`.  |

### Custom Transform Example

```python
class DiscountFutureRewards(StepRewardTransform):
    """Apply Œ≥^t discount to every intermediate reward."""
    def __init__(self, gamma: float = 0.99):
        self.gamma = gamma
    def __call__(self, trajectory, step_index, base_reward):
        return base_reward * (self.gamma ** step_index)

step_transforms = ComposeStepRewardTransforms([
    RewardForFormat(reward=0.05, penalty=-0.05),
    DiscountFutureRewards(gamma=0.97),
])
```

This first rewards/penalises formatting, then exponentially discounts by step index.

</details>

<details>
<summary><strong>Sampling‚ÄëReward¬†Transforms¬†(`transformation_sampling.py`)</strong></summary>

### API

* Sub‚Äëclass **`SamplingRewardTransform`** and implement `__call__(steps: List[Step]) -> List[Step]`.
* A stack is applied via **`ComposeSamplingRewardTransforms([...])`** *after* the batch is drawn from **StepBuffer**.

### Built‚Äëin Transforms

| Class                   | Effect                                                                 |
| ----------------------- | ---------------------------------------------------------------------- |
| `NormalizeRewards`      | Subtracts the mean reward across the batch (optionally divide by std). |
| `NormalizeRewardsByEnv` | Mean‚Äëcentres (and optionally z‚Äëscores) rewards *per environment ID*.   |

### Custom Transform Example

```python
class ClampRewards(SamplingRewardTransform):
    """Clip rewards into [min_r, max_r]."""
    def __init__(self, min_r: float = -1.0, max_r: float = 1.0):
        self.min_r, self.max_r = min_r, max_r
    def __call__(self, steps, env_id=None):
        for s in steps:
            s.reward = max(self.min_r, min(self.max_r, s.reward))
        return steps

sampling_transforms = ComposeSamplingRewardTransforms([
    NormalizeRewardsByEnv(z_score=True),
    ClampRewards(min_r=-2, max_r=2),
])
```

This normalises rewards per env and then clamps extreme values.

</details>

---

## Algorithms

### Reinforce

Explanation, use-cases, and examples.

### Extending with Custom Algorithms

How to implement and integrate custom algorithms.

---

## Utilities and Helpers

### Templates

Documentation for templates handling prompts and action extraction.

### Logging

Logging utility documentation.

---

## Configuration Reference

Comprehensive reference table for configurations and parameters.

---

## Extending the Framework

Instructions and examples for extending functionality, adding games, and writing custom components.

---

## Troubleshooting and FAQ

Common issues, questions, and solutions.

---

## Contributing

Guidelines on contributing to the project.

---

## Contact and Support

Contact information and channels for support and discussion.
